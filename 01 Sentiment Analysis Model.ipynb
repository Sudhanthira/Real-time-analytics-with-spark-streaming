{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d09556",
   "metadata": {},
   "source": [
    "# Step 1: Producing labelled tweets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced7cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "TOPIC = 'labelledtweets' # Name of the topic\n",
    "URL = 'https://drive.google.com/uc?id=1Lf4ievO9SP1DD-KIOXL8pOc36yZ88lqr' # URL where the dataset is stored\n",
    "OUTPUT = 'data/labelledtweets.csv' # Place to store the dataset after download\n",
    "ENCODER = 'ISO-8859-1' # Encoder used in the csv\n",
    "HEADERS = 'false' # The csv does not have headers\n",
    "NAMES = ['target', 'id', 'date', 'flag', 'user', 'text'] # Manually putting headers of the csv\n",
    "IDCOL = \"id\" # Column used as key\n",
    "DELAY = 0 # Delay between messages\n",
    "\n",
    "# Joining names as one string\n",
    "NAMES = \",\".join(NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a05ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the file and start producing the data to the topic\n",
    "# Careful. It takes like 3 hours\n",
    "!python producers/csvproducer.py \\\n",
    "                --topic $TOPIC \\\n",
    "                --url $URL \\\n",
    "                --output $OUTPUT \\\n",
    "                --encoder $ENCODER \\\n",
    "                --headers $HEADERS \\\n",
    "                --names $NAMES \\\n",
    "                --idcol $IDCOL \\\n",
    "                --delay $DELAY \\\n",
    "                > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e1dd4",
   "metadata": {},
   "source": [
    "# Step 2: Building Spark Streaming DF to create parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9210ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.types import MapType, StringType, IntegerType, ArrayType, StructType, \\\n",
    "                              StructField, LongType, DoubleType, BooleanType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef61cff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Spark session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('kafka') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79067607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema of the labelled tweets data\n",
    "schema_tweet = StructType([\n",
    "    StructField(\"target\", IntegerType(),  True),\n",
    "    StructField(\"id\", IntegerType(),  True),\n",
    "    StructField(\"date\", StringType(),  True),\n",
    "    StructField(\"flag\", StringType(),  True),\n",
    "    StructField(\"user\", StringType(),  True),\n",
    "    StructField(\"text\", StringType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "207403e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from the topic\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "  .option(\"subscribe\", \"labelledtweets\") \\\n",
    "  .option(\"startingOffsets\",\"earliest\") \\\n",
    "  .option(\"includeHeaders\", \"true\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fdf9e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data of the topic\n",
    "ds = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\\\n",
    "    .withColumn(\"value\", F.from_json(\"value\", schema_tweet)) \\\n",
    "    .select(F.col('value.*')) \\\n",
    "    .select('target', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36357877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the data to a parquet file\n",
    "output_ds = ds \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"data/parquet/\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint/data\") \\\n",
    "    .outputMode(\"Append\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c44eb0",
   "metadata": {},
   "source": [
    "# Step 3: Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8fddf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import cld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c86624ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To always show the results of DataFrames and improve the formatting of the output\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)\n",
    "\n",
    "## To allow future conversion of Spark DataFrame into Pandas DataFrame\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb57b11",
   "metadata": {},
   "source": [
    "##  UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b42c34",
   "metadata": {},
   "source": [
    "### Remove words that start with a character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cb5736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_start(text, start_chr):\n",
    "    '''\n",
    "    It returns the string but removing words that start with start_chr\n",
    "    '''\n",
    "    return \" \".join(filter(lambda word:word[0]!=start_chr, text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70236c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating UDF of the remove_start functio\n",
    "UDF_remove_start = F.udf(lambda text, start_chr: remove_start(text, start_chr), \n",
    "                        StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1428f4a1",
   "metadata": {},
   "source": [
    "### Detect language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6221be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lang(text):\n",
    "    '''\n",
    "    It returns the language of the text if possible\n",
    "    '''\n",
    "    try:\n",
    "        return cld3.get_language(text)[0]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70b34ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating UDF for detect_lang function\n",
    "UDF_detect_lang = F.udf(lambda text: detect_lang(text), \n",
    "                        StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128d9c9c",
   "metadata": {},
   "source": [
    "## Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e126c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding schema just in case\n",
    "parquet_schema = StructType([\n",
    "    StructField(\"target\", IntegerType(),  True),\n",
    "    StructField(\"text\", StringType(),  True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "693f894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Reading tweets data from parquets\n",
    "# 2) Removing words that strat with @\n",
    "# 3) Removing words that strat with #\n",
    "# 4) To lowercase\n",
    "# 5) Add language column\n",
    "# 6) Filter only enlish\n",
    "# 7) Remove lang column\n",
    "# 8) Remove numbers\n",
    "# 9) Remove symbols\n",
    "# 10) Keep obs with at least one letter\n",
    "\n",
    "df = spark.read.format('parquet').schema(parquet_schema).load('data/parquet/') \\\n",
    "    .withColumn('text',UDF_remove_start(F.col('text'), F.lit('@'))) \\\n",
    "    .withColumn('text',UDF_remove_start(F.col('text'), F.lit('#'))) \\\n",
    "    .withColumn('text', F.lower(F.col('text'))) \\\n",
    "    .withColumn('lang', UDF_detect_lang(F.col('text'))) \\\n",
    "    .filter(F.col('lang') == 'en') \\\n",
    "    .select('target', 'text') \\\n",
    "    .withColumn(\"text\", F.regexp_replace(F.col(\"text\"), r'[0-9]', '')) \\\n",
    "    .withColumn(\"text\", F.regexp_replace(F.col(\"text\"), r'[$-/:-?{-~!\"^_`\\[\\]]', '')) \\\n",
    "    .filter(F.col('text').rlike(\"^.*[a-zA-Z]+.*$\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "721209c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>target</th><th>text</th></tr>\n",
       "<tr><td>0</td><td>is upset that he ...</td></tr>\n",
       "<tr><td>0</td><td>i dived many time...</td></tr>\n",
       "<tr><td>0</td><td>my whole body fee...</td></tr>\n",
       "<tr><td>0</td><td>no its not behavi...</td></tr>\n",
       "<tr><td>0</td><td>not the whole crew</td></tr>\n",
       "<tr><td>0</td><td>hey long time no ...</td></tr>\n",
       "<tr><td>0</td><td>nope they didnt h...</td></tr>\n",
       "<tr><td>0</td><td>i couldnt bear to...</td></tr>\n",
       "<tr><td>0</td><td>it it counts idk ...</td></tr>\n",
       "<tr><td>0</td><td>i wouldve been th...</td></tr>\n",
       "<tr><td>0</td><td>i wish i got to w...</td></tr>\n",
       "<tr><td>0</td><td>hollis death scen...</td></tr>\n",
       "<tr><td>0</td><td>about to file taxes</td></tr>\n",
       "<tr><td>0</td><td>ahh ive always wa...</td></tr>\n",
       "<tr><td>0</td><td>oh dear were you ...</td></tr>\n",
       "<tr><td>0</td><td>i was out most of...</td></tr>\n",
       "<tr><td>0</td><td>one of my friend ...</td></tr>\n",
       "<tr><td>0</td><td>i baked you a cak...</td></tr>\n",
       "<tr><td>0</td><td>this week is not ...</td></tr>\n",
       "<tr><td>0</td><td>blagh class at  t...</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+------+--------------------+\n",
       "|target|                text|\n",
       "+------+--------------------+\n",
       "|     0|is upset that he ...|\n",
       "|     0|i dived many time...|\n",
       "|     0|my whole body fee...|\n",
       "|     0|no its not behavi...|\n",
       "|     0|  not the whole crew|\n",
       "|     0|hey long time no ...|\n",
       "|     0|nope they didnt h...|\n",
       "|     0|i couldnt bear to...|\n",
       "|     0|it it counts idk ...|\n",
       "|     0|i wouldve been th...|\n",
       "|     0|i wish i got to w...|\n",
       "|     0|hollis death scen...|\n",
       "|     0| about to file taxes|\n",
       "|     0|ahh ive always wa...|\n",
       "|     0|oh dear were you ...|\n",
       "|     0|i was out most of...|\n",
       "|     0|one of my friend ...|\n",
       "|     0|i baked you a cak...|\n",
       "|     0|this week is not ...|\n",
       "|     0|blagh class at  t...|\n",
       "+------+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e30ea6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "f7dcf662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting data into train and test set\n",
    "train, test = df.randomSplit([0.7, 0.3], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "c4af3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting stop words (not used)\n",
    "stopwords = StopWordsRemover.loadDefaultStopWords('english')\n",
    "stopwords = stopwords + [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "ac2be407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the variable stages to store all the transformations\n",
    "stages = []\n",
    "\n",
    "# Separating strings into tokens\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W+\")\n",
    "stages += [regexTokenizer]\n",
    "\n",
    "# Stop words (not used)\n",
    "# stopwordsRemover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\").setStopWords(stopwords)\n",
    "# stages += [stopwordsRemover]\n",
    "\n",
    "# Creating bigrams\n",
    "ngram = NGram(n=2, inputCol=\"tokens\", outputCol=\"ngrams\")\n",
    "stages += [ngram]\n",
    "\n",
    "# Get the frequency per bigram (top 10000 that appear at least 6 times)\n",
    "countVectors = CountVectorizer(inputCol=\"ngrams\", outputCol=\"features\", vocabSize = 10000, minDF=6.0)\n",
    "stages += [countVectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "b00e93f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model\n",
    "naiveBayes = NaiveBayes(featuresCol=\"features\", labelCol='target', smoothing=1.0, modelType=\"multinomial\")\n",
    "stages += [naiveBayes]\n",
    "\n",
    "# Joining the stages\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "31713cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "de67f44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('models/naivebayes.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78da004d",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "aafd5410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "27069679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the curve (AUC) on test data = 0.570551\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "evaluator_auc = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='target')\n",
    "\n",
    "auc = evaluator_auc.evaluate(predictions)\n",
    "print(\"Area under the curve (AUC) on test data = %g\" % auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
